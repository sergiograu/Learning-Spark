{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ca92f0",
   "metadata": {},
   "source": [
    "# SCALA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e2f625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Bloggers\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Bloggers(id: Int, first: String, last: String, url: String, date: String, hits: Int, campaigns: Array[String])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c9b60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Encoders\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8d7709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esquema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(first,StringType,true), StructField(last,StringType,true), StructField(url,StringType,true), StructField(date,StringType,true), StructField(hits,IntegerType,false), StructField(campaigns,ArrayType(StringType,true),true))\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esquema = Encoders.product[Bloggers].schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d6e4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bloggers: String = C:/LearningSparkV2-master/databricks-datasets/learning-spark-v2/blogs.json\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bloggers = \"C:/LearningSparkV2-master/databricks-datasets/learning-spark-v2/blogs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807853ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bloggersDS: org.apache.spark.sql.Dataset[Bloggers] = [id: int, first: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bloggersDS = spark.read.format(\"json\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .schema(esquema)\n",
    "                .json(bloggers)\n",
    "                .as[Bloggers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e83c2",
   "metadata": {},
   "source": [
    "###### Creating Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f233846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random._\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36730f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Usage\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Usage(uid:Int, uname:String, usage:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "178680cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r: scala.util.Random = scala.util.Random@660320c4\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r = new scala.util.Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "579f73e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: scala.collection.immutable.IndexedSeq[Usage] = Vector(Usage(0,user-Gpi2C,525), Usage(1,user-DgXDi,502), Usage(2,user-M66yO,170), Usage(3,user-xTOn6,913), Usage(4,user-3xGSz,246), Usage(5,user-2aWRN,727), Usage(6,user-EzZY1,65), Usage(7,user-ZlZMZ,935), Usage(8,user-VjxeG,756), Usage(9,user-iqf1P,3), Usage(10,user-91S1q,794), Usage(11,user-qHNj0,501), Usage(12,user-7hb94,460), Usage(13,user-bz0WF,142), Usage(14,user-71nwy,479), Usage(15,user-7GZz1,823), Usage(16,user-1CSk6,140), Usage(17,user-WPzlL,246), Usage(18,user-VaEit,451), Usage(19,user-PSaRq,679), Usage(20,user-0Kkzu,332), Usage(21,user-UN3MG,172), Usage(22,user-KwwER,442), Usage(23,user-ZnltJ,923), Usage(24,user-IRA17,741), Usage(25,user-yNHRT,299), Usage(26,user-CJY3C,996), Usage(27,user-Yq9WW,529), Usage(28,user-RFWw1,30...\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = for (i <- 0 to 1000)\n",
    " yield (Usage(i, \"user-\" + r.alphanumeric.take(5).mkString(\"\"), r.nextInt(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e74e78a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UsageDS: org.apache.spark.sql.Dataset[Usage] = [uid: int, uname: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val UsageDS = spark.createDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dfc8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|uname     |usage|\n",
      "+---+----------+-----+\n",
      "|0  |user-Gpi2C|525  |\n",
      "|1  |user-DgXDi|502  |\n",
      "|2  |user-M66yO|170  |\n",
      "|3  |user-xTOn6|913  |\n",
      "|4  |user-3xGSz|246  |\n",
      "|5  |user-2aWRN|727  |\n",
      "|6  |user-EzZY1|65   |\n",
      "|7  |user-ZlZMZ|935  |\n",
      "|8  |user-VjxeG|756  |\n",
      "|9  |user-iqf1P|3    |\n",
      "+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "UsageDS.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a816844",
   "metadata": {},
   "source": [
    "###### Higher-order functions and functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15aa83cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a72bc5fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|uname     |usage|\n",
      "+---+----------+-----+\n",
      "|634|user-L0wci|999  |\n",
      "|561|user-5n2xY|999  |\n",
      "|113|user-nnAXr|999  |\n",
      "|605|user-NL6c4|999  |\n",
      "|26 |user-CJY3C|996  |\n",
      "|345|user-QKrVb|996  |\n",
      "|805|user-LX27o|996  |\n",
      "|49 |user-xPBrB|993  |\n",
      "|142|user-waRT9|992  |\n",
      "|681|user-QwV36|992  |\n",
      "+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "UsageDS.where(col(\"usage\")>900).orderBy(desc(\"usage\")).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ffa92",
   "metadata": {},
   "source": [
    "Esto es otra manera de hacerlo, declarando una funciÃ³n previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6295cee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterWithUsage: (u: Usage)Boolean\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filterWithUsage(u:Usage) = u.usage>900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dee692f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// dsUsage.filter(filterWithUsage(_)).orderBy(desc(\"usage\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ed2c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// UsageDS.filter(d => d.usage > 900).orderBy(desc(\"usage\")).show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e78253",
   "metadata": {},
   "source": [
    "USANDO EL IF ELSE EN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aeef0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// UsageDS.map(u => {if (u.usage > 750) u.usage * .15 else u.usage * .50 }).show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63a11788",
   "metadata": {},
   "outputs": [],
   "source": [
    "// UsageDS.when(col(\"usage\")>750, col(\"usage\")* 0.15).when(col(\"usage\")<750, col(\"usage\")* 0.50).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228d9f4",
   "metadata": {},
   "source": [
    "Como las funciones lambda no funcionan en Jupiter Notebook, hemos creado una vista temporal y hemos usado spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "371f4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "UsageDS.createOrReplaceTempView(\"dsUsage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c240140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+----------+\n",
      "|uid|     uname|usage|usage_mult|\n",
      "+---+----------+-----+----------+\n",
      "|561|user-5n2xY|  999|    149.85|\n",
      "|113|user-nnAXr|  999|    149.85|\n",
      "|605|user-NL6c4|  999|    149.85|\n",
      "|634|user-L0wci|  999|    149.85|\n",
      "|345|user-QKrVb|  996|    149.40|\n",
      "|805|user-LX27o|  996|    149.40|\n",
      "| 26|user-CJY3C|  996|    149.40|\n",
      "| 49|user-xPBrB|  993|    148.95|\n",
      "|255|user-Ckzof|  992|    148.80|\n",
      "|681|user-QwV36|  992|    148.80|\n",
      "+---+----------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT uid, uname, usage, CASE \n",
    "    WHEN usage > 750 THEN usage * 0.15\n",
    "    WHEN usage < 750 THEN usage * 0.15\n",
    "    ELSE 750 *1\n",
    "END AS usage_mult\n",
    "FROM dsUsage\n",
    "ORDER BY usage DESC\"\"\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
