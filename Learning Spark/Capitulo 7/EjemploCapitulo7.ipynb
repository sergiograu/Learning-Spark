{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0234624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002861.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1623831370539)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|key                                                      |value                                                           |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes          |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>|\n",
      "|spark.sql.adaptive.coalescePartitions.enabled            |true                                                            |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum|<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionNum    |<undefined>                                                     |\n",
      "|spark.sql.adaptive.enabled                               |false                                                           |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SET -v\"\"\").select(\"key\", \"value\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c165a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: String = 200\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52345d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5bfee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: String = 5\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56db7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0902cef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: String = 20\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08a2c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: bigint, square: bigint]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", $\"id\" * $\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05e2ab",
   "metadata": {},
   "source": [
    "Cuando un dataframe de mucho tamaño lo guardamos en la memoria cache, mejoramos su posterior uso ya que se reducira el tiempo en ejecución de los comandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deeee594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: df.type = [id: bigint, square: bigint]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d751849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Long = 10000000\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1998039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 10000000\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799a8e6",
   "metadata": {},
   "source": [
    "Lo mismo vemos que pasa usando la funcion persist(), esta funcion hace lo mismo que el cache pero puede controlar donde y como se guardan las cosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee0a9764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd61d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [id: bigint, square: bigint]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.range(1 * 100000000).toDF(\"id\").withColumn(\"square\", $\"id\" * $\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd4ed887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: df1.type = [id: bigint, square: bigint]\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0285f19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Long = 100000000\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f36347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Long = 100000000\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbc229de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: df1.type = [id: bigint, square: bigint]\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e05c7",
   "metadata": {},
   "source": [
    "Shuffle Sort Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38b4d6",
   "metadata": {},
   "source": [
    "Unir dos dataset de gran tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93475c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e820458e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = 10485760b\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4290f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09dbeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: String = -1\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9c3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13ee2b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: String = 10485760b\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d81cd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "states: scala.collection.mutable.Map[Int,String] = Map()\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var states = scala.collection.mutable.Map[Int, String]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29fbefe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "items: scala.collection.mutable.Map[Int,String] = Map()\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var items = scala.collection.mutable.Map[Int, String]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bc48a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rnd: scala.util.Random = scala.util.Random@587a3024\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rnd = new scala.util.Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b7a81d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: scala.collection.mutable.Map[Int,String] = Map(2 -> CA, 5 -> MI, 4 -> NY, 1 -> CO, 3 -> TX, 0 -> AZ)\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states += (0 -> \"AZ\", 1 -> \"CO\", 2-> \"CA\", 3-> \"TX\", 4 -> \"NY\", 5-> \"MI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95002d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: scala.collection.mutable.Map[Int,String] = Map(2 -> SKU-2, 5 -> SKU-5, 4 -> SKU-4, 1 -> SKU-1, 3 -> SKU-3, 0 -> SKU-0)\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items += (0 -> \"SKU-0\", 1 -> \"SKU-1\", 2-> \"SKU-2\", 3-> \"SKU-3\", 4 -> \"SKU-4\", 5-> \"SKU-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a25d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usersDF: org.apache.spark.sql.DataFrame = [uid: int, login: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val usersDF = (0 to 1000000).map(id => (id, s\"user_${id}\",\n",
    " s\"user_${id}@databricks.com\", states(rnd.nextInt(5))))\n",
    " .toDF(\"uid\", \"login\", \"email\", \"user_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bbcdb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersDF: org.apache.spark.sql.DataFrame = [transaction_id: int, quantity: int ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersDF = (0 to 1000000)\n",
    " .map(r => (r, r, rnd.nextInt(10000), 10 * r* 0.2d,\n",
    " states(rnd.nextInt(5)), items(rnd.nextInt(5))))\n",
    " .toDF(\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4bca480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usersOrdersDF: org.apache.spark.sql.DataFrame = [transaction_id: int, quantity: int ... 8 more fields]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val usersOrdersDF = ordersDF.join(usersDF, $\"users_id\" === $\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cda7b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": " Java heap space\r",
     "output_type": "error",
     "traceback": [
      "java.lang.OutOfMemoryError: Java heap space\r",
      ""
     ]
    }
   ],
   "source": [
    "usersOrdersDF.show(2, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd4514",
   "metadata": {},
   "source": [
    "Examining our final execution plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d877c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// usersOrderDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "038db74b",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": " Java heap space\r",
     "output_type": "error",
     "traceback": [
      "java.lang.OutOfMemoryError: Java heap space\r",
      ""
     ]
    }
   ],
   "source": [
    "usersDF.show(10, false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
